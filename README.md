# 밑바닥부터 시작하는 딥러닝 정리

## Chapter 1 헬로 파이썬

- 이 책에서는 다음의 프로그래밍 언어와 라이브러리를 사용한다.
    - python 3
    - numpy
    - matplotlib
- 자료형
    - type() 함수로 특정 데이터의 자료형을 알아볼 수 있다.
    - 리스트 = []
    - 딕셔너리 = { key : value }
    - bool = True or False
- 조건문 if
- 반복문 for
- def 로 함수 정의
- class
    - `__init__` 이라는 특별한 메서드를 이용하여 클래스를 초기화하는 방법을 정의한다.
- numpy
    - `import numpy as np`
    - `np.array()` > numpy 배열 생성
    - `a.flatten()` > a를 1차원 배열로 변환(평탄화)
    - `np.arrange(start, end, step)` > start 부터 end까지 step 간격으로 생성
- matplotlib
    - `import matplotlib.pyplot as plt`
    - `plt.plot(x, y, [linestyle="--"], [label="cos"])` > x, y 로 그래프 그리기
    - `plt.xlabel("x")` > x축 이름
    - `plt.ylabel("y")` > y축 이름
    - `plt.title('x & y')` > 그래프 제목
    - `plt.legend()` > 범례 생성
    - `plt.show()` > 그려진 그래프 시각화해서 보여주기
- image 관련 함수
    - `from matplotlib.image import imread`
    - `imread('asdf.png')`
    - `plt.imshow(img)`
    - `plt.show()`

## Chapter 2 퍼셉트론

- **퍼셉트론** : 다수의 신호를 입력받아 하나의 신호를 출력하는 알고리즘

    $$⁍$$

- 단순한 논리 회로
    - AND
        - 두 입력이 모두 1일때 1을 출력
    - NAND (Not AND)
        - 두 입력이 모두 0일때 1을 출력
    - OR
        - 둘 중 하나라도 1이면 1을 출력
    - XOR
        - 어느 한 쪽이 1일때만 1을 출력
- 퍼셉트론의 한계
    - 단층 퍼셉트론으로는 XOR 게이트를 표현할 수 없다.
    - 단층 퍼셉트론으로는 비선형 영역을 분리할 수가 없다.
    - NAND + OR > AND > XOR
- NAND 게이트의 조합만으로 컴퓨터를 만들 수 있다 > 이론상 2층 퍼셉트론이면 컴퓨터를 만들 수 있다.

## Chapter 3 신경망

- **신경망**은 입력층, 은닉층, 출력층으로 구성되어 있다.
- **은닉층**의 뉴런은 사람 눈에는 보이지 않는다.
- **편향**(**b**, bias)은 뉴런이 얼마나 쉽게 활성화되느냐를 제어하는 매개변수이다.

    $$⁍$$

- 계단함수

    $$h(x) = \begin{cases}
       0(x \le 0) \\
       1(x \gt 0)
    \end{cases}$$

- **활성화 함수(Active function)**는 $h(x)$와 같이 입력 신호의 총합을 출력 신호로 변환하는 함수이다.
- 시그모이드 함수

    $$h(x) = \frac
    {1} 
    {1 + e^{-x}}$$

- 시그모이드 함수와 계단함수의 비교
    - 두 함수 모두 입력이 작을 때에는 출력이 0에 가깝고, 입력이 클 때에는 출력이 1에 가깝다.
    - 시그모이드 함수는 부드러운 곡선이며 입력에 따라 출력이 연속적으로 변화한다.
    - 계단함수는 0을 경계로 출력이 급격히 변화한다.
- ReLU(Rectified Linear Unit) 함수

    $$h(x) = \begin{cases}
       0(x \le 0) \\
       x(x \gt 0)
    \end{cases}$$

- 다차원 배열의 계산
    - 행렬을 곱할때에는 대응하는 차원의 원소 수를 일치시켜 주어야 한다.
- 출력층 설계하기
    - 항등 함수(identity function)
        - 입력을 그대로 출력하는 함수
    - 소프트맥스 함수(softmax function)

        $$y_k = \frac
        {\exp(a_k)} 
        {\displaystyle\sum_{i=1}^n \exp(a_i)}$$

        - 소프트맥스 함수는 지수 함수이기 때문에 쉽게 큰 값에 도달하므로 개선된 수식을 사용한다. 여기에서 $C'$ 는 일반적으로 최댓값을 사용한다.

        $$y_k = \frac
        {\exp(a_k+C')} 
        {\displaystyle\sum_{i=1}^n \exp(a_i+C')}$$

        - 소프트맥스 함수의 출력은 0 ~ 1.0 사이의 실수이며, 출력의 총합은 1이다. 따라서 소프트맥스 함수의 출력을 '확률'로 해석할 수 있다.
        - 그러나 소프트맥스 함수를 적용하여도 출력이 가장 큰 뉴런의 위치는 달라지지 않기 때문에 신경망으로 분류할 때에는 추론 단계에서는 출력층의 소프트맥스 함수를 생략해도 된다.
- MNIST 데이터셋
    - 손글씨 숫자 이미지 집합. 0부터 9까지 28x28 pixel의 훈련 이미지가 6만장, 시험 이미지가 1만장 준비되어 있다.
    - 이 데이터셋을 사용하여 모델을 학습하고, 학습한 모델로 얼마나 정확하게 분류하는지를 평가한다.
- **정규화(Normalization)**는 데이터를 특정 범위로 변환하는 처리이다.
- **배치(batch)**는 하나로 묶은 입력 데이터를 말한다. 배치 처리를 하게 되면 계산 속도가 빨라진다.

## Chapter 4 신경망 학습

- 신경망의 특징은 데이터를 보고 학습할 수 있다는 점이다.
- **오버피팅(overfitting)**은 한 데이터셋에만 지나치게 최적화된 상태를 말한다.
- **손실 함수(loss function)**는 최적의 매개변수 값을 탐색하기 위한 지표이다.
    - 오차제곱합(Sum of Squares for Error, SSE)은 가장 많이 쓰이는 손실함수이다.

    $$⁍$$

    - 교차 엔트로피 오차(Cross Entropy Error, CEE)
    - 교차 엔트로피 오차는 $y=\log x$의 그래프로 나타낼 수 있으며, $x$가 1일 때 $y$는 0이고 $x$가 0에 가까울 수록 $y$의 값은 점점 작아진다

    $$E = \displaystyle- 
    {\displaystyle\sum_{k}t_k\log y_k}$$

- **미니배치(mini-batch)**는 훈련 데이터로부터 일부만 골라 학습하는 것을 말한다. 미니배치를 사용하는 이유는 모든 데이터를 대상으로 손실 함수의 합을 구하려면 시간이 걸리기 때문이다.
- 손실 함수를 지표로 하는 이유는 매개변수가 조금만 바뀌어도 정확도에 비해서 연속적으로 변하기 때문이다.
- **수치 미분**은 **근사치**로 계산하는 방법이다.
    - 근사치를 구하기 위해 $h$는 매우 작은 값을 사용하지만 반올림 오차 문제를 일으키기 때문에 $h = 10^{-4}$ 정도의 값을 사용한다.
    - 기울기를 구할 때 $x$에서의 기울기를 구해야 하지만 실제로는 $f(x+h)$와 $f(x)$ 사이의 기울기를 구하는 것이 된다. 따라서 이 오차를 줄이기 위해 $f(x+h)$와 $f(x-h)$일 때의 함수 f의 차분을 계산하는 방법을 사용하기도 한다. 이를 **중심 차분** 또는 **중앙 차분**이라고 한다.
- **기울기(gradient)**는 모든 변수의 편미분을 벡터로 정리한 것이다.
- 벡터장에서 기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향이다.
- **경사법(gradient method)**는 기울어진 방향으로 일정 거리만큼 이동하여 기울기를 구하는 것을 반복하여 함수의 값을 점차 줄이는 방법이다. 경사법은 기계학습을 최적화하는 데 흔히 쓰는 방법이다.

$$x_0=x_0-\eta\displaystyle\frac {\partial f}{\partial x_0} \\
x_1=x_1-\eta\displaystyle\frac {\partial f}{\partial x_1}$$

- 기호 $**\eta$(eta, 에타)**는 갱신하는 양을 나타낸다. 이를 신경망 학습에서는 **학습률(learning rate)**라고 한다.
- 학습률과 같은 매개변수를 **하이퍼파라미터(Hyper Parameter, 초매개변수)**라고 하며, 이 매개변수는 사람이 직접 설정해주어야 한다.
- 신경망에서의 기울기는 가중치 매개변수에 대한 손실 함수의 기울기를 이야기 하며, 가중치가 $W$, 손실 함수가 $L$인 신경망일 때 경사는 $\frac {\partial L} {\partial W}$로 나타낼 수 있다.
- 학습 알고리즘 구현하기
    - 전제 : 신경망에는 적응 가능한 자궁치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라고 한다.
    - 1단계 - 미니배치

        훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 목표이다.

    - 2단계 - 기울기 산출

        미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기의 손실 함수 값을 가장 작게 하는 방향을 제시한다.

    - 3단계 - 매개변수 갱신

        가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.

    - 4단계 - 반복

        1~3단계를 반복한다.

- 데이터를 미니배치로 무작위로 선정하기 때문에 **확률적 경사 하강법(Stochastic Gradient Descent, SGD)**라고 부른다.
- 에폭(epoch)은 하나의 단위이다. 1에폭은 학습에서 훈련 데이터를 모두 소진했을 때의 횟수에 해당된다.

## Chapter 5 오차역전파법

- **오차역전파법(backpropagation)**은 가중치 매개변수의 기울기를 효율적으로 구하는 방법이다.
- 계산 그래프는 계산 과정을 그래프로 나타낸 것이다. 계산 그래프를 이용하면 수식을 시각적으로 더 알아보기 쉽다.
- 계산 그래프에서 계산을 왼쪽에서 오른쪽으로 진행하는 단계를 **순전파(forward propagation)**이라고 하며, 반대 방향은 **역전파(backward propagation)**이라고 한다.
- 덧셈 노드의 역전파는 미분값이 모두 1이 된다.

    $$\displaystyle\frac {\partial z} {\partial x} = 1 \\
    \displaystyle\frac {\partial z} {\partial y} = 1$$

- 곱셈 노드의 역전파는 순전파 때의 입력  신호를 서로 바꾼 값을 곱한다.

    $$\displaystyle\frac {\partial z} {\partial x} = y \\
    \displaystyle\frac {\partial z} {\partial y} = x$$

- 활성화 함수 계층 구현하기
    - ReLU 계층의 수식은 다음과 같다.

        $$y = \begin{cases}
           x(x \gt 0) \\
           0(x \le 0)
        \end{cases}$$

    - $x$에 대한 $y$의 미분은 다음과 같다.

        $$\displaystyle\frac {\partial y} {\partial x} = \begin{cases}
           1(x \gt 0) \\
           0(x \le 0)
        \end{cases}$$

    - Sigmoid 계층의 수식은 다음과 같다.

        $$y = \frac
        {1} 
        {1 + e^{-x}}$$

    - Sigmoid 계층의 계산 그래프를 간소화 한 수식은 다음과 같다.

        $$\frac {\partial L} {\partial y} y^2e^{-x} = \frac {\partial L} {\partial y} y(1-y)$$

    - 이처럼 Sigmoid 계층의 역전파는 순전파의 출력(y)만으로 계산할 수 있다.
- **Affine 계층**은 신경망의 순전파 때 수행하는 행렬의 곱을 기하학에서는 **어파인 변환(Affine transformation)**이라고 하는데, 이 어파인 변환을 수행하는 처리를 말한다.
- **Softmax-with-Loss 계층**
    - Softmax 계층의 역전파는 $(y_1-t_1,\ y_2-t_2,\ y_3-t_3)$라는 말끔한 결과를 내놓는다.
    - 즉, Softmax 계층의 출력과 정답 레이블의 차이이다.
- 오차역전파법으로 구한 기울기를 검증하기 위해 수치 미분의 결과와 오차역전파법의 결과를 비교하는데, 이것을 **기울기 확인(Gradient check)**라고 한다.

## Chapter 6 학습 관련 기술들

- **최적화(Optimization)**는 매개변수의 최적값을 찾기 위한 방법이다.
    - **확률적 경사 하강법(Stochastic Gradient Descent, SGD)**
        - 기울어진 방향으로 일정 거리만 가는 단순한 방법이며, 수식으로는 다음과 같이 나타낼 수 있다.

        $$W \leftarrow W - \eta\frac{\partial L}{\partial W}$$

        - $W$는 갱신할 가중치 매개변수, $\frac{\partial L}{\partial W}$은 W에 대한 손실 함수의 기울기, $\eta$는 학습률을 의미하며, 실제로 0.01이나 0.001과 같은 값을 미리 정해서 사용한다.
        - SGD의 단점은 비등방성 함수에 대해서는 탐색 경로가 비효율적이다.
    - **모멘텀(Momentum)**
        - '운동량'을 뜻하는 단어로, 기울기 방향으로 힘을 받아 물체가 가속된다는 물리 법칙을 나타낸다.

        $$v \leftarrow \alpha v - \eta\frac{\partial L}{\partial W} \\
        W \leftarrow W + v$$

    - **AdaGrad**
        - 신경망에서는 학습률 값이 너무 작으면 학습 시간이 너무 길어지고, 반대로 너무 크면 발산하여 학습이 제대로 이루어지지 않는다.
        - **학습률 감소(learning rate decay)**는 학습을 진행하면서 학습률을 점차 줄여가는 방법으로, 실제 신경망 학습에 자주 쓰인다.

            $$h \leftarrow h + \frac{\partial L}{\partial W} \odot\frac{\partial L}{\partial W} \\ 
            W \leftarrow W - \eta \frac{1} {\sqrt{h}} \frac{\partial L}{\partial W}$$

        - AdaGrad는 개별 매개변수에 적응적으로 학습률을 조정하면서 학습을 진행한다.
        - AdaGrad는 과거의 기울기를 제곱하여 계속 더해가기 때문에 학습을 진행할 수록 갱신 강도가 약해지기 때문에 무한히 계속 학습한다면 어느 순간 갱신량이 0이 되어버린다.
    - **RMSProp**
        - AdaGrad를 개선한 기법으로, 과거의 기울기는 잊고 새로운 기울기 정보를 크게 반영하는 **지수이동평균(Exponential Moving Average, EMA)**을 이용한다.
    - **Adam**
        - 모멘텀과 AdaGrad를 융합한 듯한 방법이다.
        - 하이퍼파라미터의 '편향 보정'이 진행되는 것이 특징이다.
- 가중치의 초깃값
    - **가중치 감소(weight decay)**는 오버피팅을 억제해 범용 성능을 높이는 테크닉이다.
    - 그렇다고 해서 가중치의 초깃값을 모두 0(또는 균일한 값)으로 설정하면 학습이 올바로 이루어지지 않는다. 이유는 모든 가중치의 값이 똑같이 갱신되기 때문이다. 따라서 가중치의 초깃값은 무작위로 설정해야 한다.
    - **기울기 소실(Gradient vanishing)**은 데이터가 0과 1에 치우쳐 분포할 때, 역전파의 기울기 값이 점점 작아지다가 사라지는 현상을 말한다.
    - **Xavier 초깃값**은 $n$개의 노드가 있을 때 표준편차가 $\frac {1} {\sqrt{n}}$인 분포를 사용하는 것이다.
        - Xavier 초깃값은 층이 깊어질 수록 치우침이 조금씩 커지기 때문에 학습할 때 기울기 소실 문제를 일으킨다.
    - **He 초깃값**은 n개의 노드가 있을 때 표준편차가$\sqrt\frac {2} {n}$인 정규분포를 사용하는 것이다.
    - 활성화 함수로 ReLU를 사용할 때에는 He 초깃값을, Sigmoid나 tanh 등의 S자 모양 곡선일 때에는 Xavier 초깃값을 쓰는 것이 좋다.
- **배치 정규화(Batch Normalization)**는 활성화값 분포를 적절하게 분배하는 방법이다.
    - 배치 정규화의 장점
        - 학습을 빨리 진행할 수 있다.
        - 초깃값에 크게 의존하지 않는다.
        - 오버피팅을 억제한다.

    $$\mu_B \leftarrow \displaystyle\frac {1}{m} 
    {\displaystyle\sum_{i=1}^m x_i} \\
    \sigma_B^2 \leftarrow \displaystyle\frac {1}{m} 
    {\displaystyle\sum_{i=1}^m (x_i-\mu_B)^2} \\
    \hat{x}_i \leftarrow \frac {x_i-\mu_B} {\sqrt{\sigma^2_B+\epsilon}}$$

    - $\epsilon$은 작은 값(ex $10^{-7}$)을 넣어서 0으로 나누는 것을 예방하는 역할이다.
    - 정규화된 데이터에 고유한 확대(scale)와 이동(shift) 변환을 수행하며 수식은 다음과 같다.

    $$y_i \leftarrow \gamma\hat{x}_i+\beta$$

- **오버피팅(Overfitting)**은 신경망이 훈련 데이터만 지나치게 적응되어 그 외에 데이터에는 제대로 대응하지 못하는 상태를 말한다.
    - 오버피팅이 잘 일어나는 경우
        - 매개변수가 많고 표현력이 높은 모델
        - 훈련 데이터가 적은 모델
    - 오버피팅을 억제하는 방법
        - **가중치 감소(Weight decay)**
            - 가중치 감소는 모든 가중치 각각의 손실 함수에 $\frac{1}{2}\lambda W^2$을 더하여 가중치가 커지는 것을 억제한다.
        - **드롭아웃(Dropout)**
            - 훈련 시에 뉴런을 임의로 삭제하면서 학습하는 방법이다.
        - **앙상블 학습(Ensemble learning)**
            - 개별적으로 학습시킨 여러 모델의 출력을 평균내어 추론하는 방식으로 드롭아웃과 같은 효과를 낸다.
- 적절한 하이퍼파라미터 값 찾기
    - 각 층의 뉴런 수, 배치 크기, 매개변수 갱신 시의 학습률과 가중치 감소 등
    - 하이퍼파라미터의 성능을 평가할 때는 **검증 데이터(validation data)**를 사용해야 한다. 왜냐하면 시험 데이터를 사용하여 하이퍼파라미터를 조정하면 하이퍼파라미터 값이 시험 데이터에 오버피팅되기 때문이다.
    - 하이퍼파라미터를 최적화 할 때의 핵심은 하이퍼파라미터의 '최적 값'이 존재하는 범위를 조금씩 줄여나가는 것이다.
    - 0단계

        하이퍼파라미터 값의 범위를 설정한다.

    - 1단계

        설정된 범위에서 하이퍼파라미터의 값을 무작위로 추출한다.

    - 2단계

        1단계에서 샘플링한 하이퍼파라미터 값을 사용하여 학습하고, 검증 데이터로 정확도를 평가한다. (단, 에폭은 작게 설정한다)

    - 3단계

        1단계와 2단계를 특정 횟수(100회 등) 반복하며, 그 정확도의 결과를 보고 하이퍼파라미터의 범위를 좁힌다.

    - **베이즈 최적화(Bayesian optimization)**로 더욱 효과적인 하이퍼파라미터를 찾을 수 있다.

## Chapter 7 합성곱 신경망(CNN)

- 합성곱 신경망(Convolutional Neural Network, CNN)은 이미지 인식과 음성 인식 등 다양한 곳에서 사용되는 딥러닝 기법이다.
- CNN에서는 새로운 '합성곱 계층(Conv)'과 '풀링 계층(Pooling)'이 추가된다.
- CNN에서는 합성곱 계층의 입출력 데이터를 **특징 맵(feature map)**이라고도 한다. 입력 데이터를 **입력 특징 맵(Input feature map)**, 출력 데이터를 **출력 특징 맵(Output feature map)**이라고 한다.
- 합성곱 연산은 필터의 윈도우를 일정 간격으로 이동해가며 입력 데이터에 적용한다. 입력과 필터에서 대응하는 원소끼리 곱한 후 그 총합을 구하며, 이 계산을 **단일 곱센-누산(Fused Multiply-Add, FMA)**이라고 한다.
- **패딩(Padding)**은 합성곱 연산을 수행하기 전에 입력 데이터 주변을 특정 값(ex. 0)으로 채우는 것을 말한다. 패딩은 주로 출력 크기를 조정할 목적으로 사용한다.
- **스트라이드(Stride)**는 필터를 적용하는 위치의 간격을 말한다.
    - 입력 크기를 $(H, W)$, 필터 크기를 $(FH, FW)$, 출력 크기를 $(OH, OW)$, 패딩을 $P$, 스트라이드를 $S$라 하면, 출력 크기는 다음과 같이 계산한다.

    $$OH=\frac {H+2P-FH} {S} +1 \\
    OW=\frac {W+2P-FW} {S} +1$$

- **풀링(Pooling)**은 가로 세로 방향의 공간을 줄이는 연산이다.
    - **최대 풀링(Max Pooling)**은 최댓값을 구하는 연산이다.
    - **평균 풀링(Average Pooling)**은 평균값을 구하는 연산이다.
    - 풀링의 윈도우 크기와 스트라이드는 같은 값으로 설정하는 것이 보통이다.
- 풀링 계층의 특징
    - 학습해야 할 매개변수가 없다
    - 채널 수가 변하지 않는다
    - 입력의 변화에 영향을 적게 받는다
- im2col 함수를 이용하면 합성곱 계층과 풀링 계층을 간단하고 효율적으로 구현할 수 있다.
- CNN 시각화하기
    - 합성곱 계층의 필터에서는 특징적인 정보를 추출할 수 있다.
    - 1번째 층의 합성곱 계층에서는 에지나 블롭 등의 저수준 정보가 추출되고, 계층을 여러 겹 쌓으면 층이 깊어지면서 더 복잡하고 추상화된 정보가 추출된다.
- 대표적인 CNN
    - LeNet
        - 손글씨 숫자를 인식하는 네트워크
        - 활성화 함수로 Sigmoid 함수를 사용
        - 서브샘플링을 하여 중간 데이터 크기를 줄임
    - AlexNet
        - 활성화 함수로 ReLU를 사용
        - 최대 풀링이 주류
        - LRN(Local Response Normalization)이라는 국소적 정규화를 실시하는 계층을 이용
        - 드롭아웃을 사용한다.

## Chapter 8 딥러닝

- 딥러닝은 층을 깊게 한 심층 신경망이다.
- **데이터 확장(Data Augmentation)**은 입력 이미지(훈련 이미지)를 알고리즘을 동원해 '인위적'으로 확장하는 것을 말한다. 입력 이미지를 회전하거나 세로로 이동하는 등 미세한 변화를 주어 이미지의 개수를 늘린다. 이는 데이터가 몇 개 없을 때 특히 효과적이다.
- 층을 깊게 하는 것의 중요성
    - 층을 깊게 할 때, 신경망의 매개변수  수가 줄어든다.(ex. 5x5 합성곱 연산 1회는 3x3x2로 대체할 수 있다.) → 학습 데이터의 양을 줄여 학습을 고속으로 수행할 수 있다.
    - 정보를 계층적으로 전달할 수 있다.
- 딥러닝의 초기 역사
    - **이미지넷(ImageNet) :** 100만장이 넘는 이미지를 담고 있는 데이터셋이다.
    - **VGG** : 합성곱 계층과 풀링 계층으로 구성되는 '기본적'인 CNN이다. VGG는 구성이 간단하여 응용하기 좋다.
    - **GoogLeNet** : 가로 방향에 '폭'이 있는 인셉션 구조를 가지고 있다.
    - **ResNet(Residual Network)** : 마이크로소프트의 팀이 개발한 네트워크이다. 입력 데이터를 합성곱 계층을 건너뛰어 출력에 바로 더하는 스킵 연결(Skip connection)을 도입하여 층의 깊이에 비례헤 성능을 향상시킬 수 있게 했다.
- GPU를 활용한 고속화
    - GPU는 병렬 수치 연산을 고속으로 처리할 수 있기 때문에 딥러닝에서의 대량의 단일 곱셈-누산(또는 큰 행렬의 곱)을 수행하는데 효율적이다.
    - 비트 정밀도를 감소시켜 연산 속도를 올릴 수 있다.
- 딥러닝의 활용
    - **사물 검출** : 이미지 속에 담긴 사물의 위치와 종류(클래스)를 알아내는 기술이다. **R-CNN(Regions with Convolutional Neural Network)**가 유명하다.
    - **분할(Segmentation)** : 이미지를 픽셀 수준에서 분류하는 문제. 픽셀 단위로 객체마다 채색된 지도(Supervised) 데이터를 사용해 학습한다.
    - **사진 캡션 생성** : 컴퓨터 비전과 자연어를 융합하여 사진을 주면 그 사진을 설명하는 글을 자동으로 생성한다. **NIC(Neural Image Caption)** 모델이 대표적이며, NIC는 심층 CNN과 자연어를 다루는 **순환 신경망(Recurrent Neural Network, RNN)**으로 구성된다.
- 딥러닝의 미래
    - 이미지 스타일(화풍) 변환
    - 이미지 생성
        - GAN(Generative Adversarial Network)이라고 하는 생성자와 식별자로 불리는 2개의 신경망을 이용한다.
    - 자율주행
    - Deep Q-Network(강화학습)
