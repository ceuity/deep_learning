# 밑바닥부터 시작하는 딥러닝

## Chapter 1 헬로 파이썬

- 이 책에서는 다음의 프로그래밍 언어와 라이브러리를 사용한다.
    - python 3
    - numpy
    - matplotlib
- 자료형
    - type() 함수로 특정 데이터의 자료형을 알아볼 수 있다.
    - 리스트 = []
    - 딕셔너리 = { key : value }
    - bool = True or False
- 조건문 if
- 반복문 for
- def 로 함수 정의
- class
    - `__init__` 이라는 특별한 메서드를 이용하여 클래스를 초기화하는 방법을 정의한다.
- numpy
    - `import numpy as np`
    - `np.array()` > numpy 배열 생성
    - `a.flatten()` > a를 1차원 배열로 변환(평탄화)
    - `np.arrange(start, end, step)` > start 부터 end까지 step 간격으로 생성
- matplotlib
    - `import matplotlib.pyplot as plt`
    - `plt.plot(x, y, [linestyle="--"], [label="cos"])` > x, y 로 그래프 그리기
    - `plt.xlabel("x")` > x축 이름
    - `plt.ylabel("y")` > y축 이름
    - `plt.title('x & y')` > 그래프 제목
    - `plt.legend()` > 범례 생성
    - `plt.show()` > 그려진 그래프 시각화해서 보여주기
- image 관련 함수
    - `from matplotlib.image import imread`
    - `imread('asdf.png')`
    - `plt.imshow(img)`
    - `plt.show()`

## Chapter 2 퍼셉트론

- **퍼셉트론** : 다수의 신호를 입력받아 하나의 신호를 출력하는 알고리즘

    $$⁍$$

- 단순한 논리 회로
    - AND
        - 두 입력이 모두 1일때 1을 출력
    - NAND (Not AND)
        - 두 입력이 모두 0일때 1을 출력
    - OR
        - 둘 중 하나라도 1이면 1을 출력
    - XOR
        - 어느 한 쪽이 1일때만 1을 출력
- 퍼셉트론의 한계
    - 단층 퍼셉트론으로는 XOR 게이트를 표현할 수 없다.
    - 단층 퍼셉트론으로는 비선형 영역을 분리할 수가 없다.
    - NAND + OR > AND > XOR
- NAND 게이트의 조합만으로 컴퓨터를 만들 수 있다 > 이론상 2층 퍼셉트론이면 컴퓨터를 만들 수 있다.

## Chapter 3 신경망

- **신경망**은 입력층, 은닉층, 출력층으로 구성되어 있다.
- **은닉층**의 뉴런은 사람 눈에는 보이지 않는다.
- **편향**(**b**, bias)은 뉴런이 얼마나 쉽게 활성화되느냐를 제어하는 매개변수이다.

    $$⁍$$

- 계단함수

    $$h(x) = \begin{cases}
       0(x \le 0) \\
       1(x \gt 0)
    \end{cases}$$

- **활성화 함수(Active function)**는 $h(x)$와 같이 입력 신호의 총합을 출력 신호로 변환하는 함수이다.
- 시그모이드 함수

    $$h(x) = \frac
    {1} 
    {1 + e^{-x}}$$

- 시그모이드 함수와 계단함수의 비교
    - 두 함수 모두 입력이 작을 때에는 출력이 0에 가깝고, 입력이 클 때에는 출력이 1에 가깝다.
    - 시그모이드 함수는 부드러운 곡선이며 입력에 따라 출력이 연속적으로 변화한다.
    - 계단함수는 0을 경계로 출력이 급격히 변화한다.
- ReLU(Rectified Linear Unit) 함수

    $$h(x) = \begin{cases}
       0(x \le 0) \\
       x(x \gt 0)
    \end{cases}$$

- 다차원 배열의 계산
    - 행렬을 곱할때에는 대응하는 차원의 원소 수를 일치시켜 주어야 한다.
- 출력층 설계하기
    - 항등 함수(identity function)
        - 입력을 그대로 출력하는 함수
    - 소프트맥스 함수(softmax function)

        $$y_k = \frac
        {\exp(a_k)} 
        {\displaystyle\sum_{i=1}^n \exp(a_i)}$$

        - 소프트맥스 함수는 지수 함수이기 때문에 쉽게 큰 값에 도달하므로 개선된 수식을 사용한다. 여기에서 $C'$ 는 일반적으로 최댓값을 사용한다.

        $$y_k = \frac
        {\exp(a_k+C')} 
        {\displaystyle\sum_{i=1}^n \exp(a_i+C')}$$

        - 소프트맥스 함수의 출력은 0 ~ 1.0 사이의 실수이며, 출력의 총합은 1이다. 따라서 소프트맥스 함수의 출력을 '확률'로 해석할 수 있다.
        - 그러나 소프트맥스 함수를 적용하여도 출력이 가장 큰 뉴런의 위치는 달라지지 않기 때문에 신경망으로 분류할 때에는 추론 단계에서는 출력층의 소프트맥스 함수를 생략해도 된다.
- MNIST 데이터셋
    - 손글씨 숫자 이미지 집합. 0부터 9까지 28x28 pixel의 훈련 이미지가 6만장, 시험 이미지가 1만장 준비되어 있다.
    - 이 데이터셋을 사용하여 모델을 학습하고, 학습한 모델로 얼마나 정확하게 분류하는지를 평가한다.
- **정규화(Normalization)**는 데이터를 특정 범위로 변환하는 처리이다.
- **배치(batch)**는 하나로 묶은 입력 데이터를 말한다. 배치 처리를 하게 되면 계산 속도가 빨라진다.

## Chapter 4 신경망 학습

- 신경망의 특징은 데이터를 보고 학습할 수 있다는 점이다.
- **오버피팅(overfitting)**은 한 데이터셋에만 지나치게 최적화된 상태를 말한다.
- **손실 함수(loss function)**는 최적의 매개변수 값을 탐색하기 위한 지표이다.
    - 오차제곱합(Sum of Squares for Error, SSE)은 가장 많이 쓰이는 손실함수이다.

    $$⁍$$

    - 교차 엔트로피 오차(Cross Entropy Error, CEE)
    - 교차 엔트로피 오차는 $y=\log x$의 그래프로 나타낼 수 있으며, $x$가 1일 때 $y$는 0이고 $x$가 0에 가까울 수록 $y$의 값은 점점 작아진다

    $$E = \displaystyle- 
    {\displaystyle\sum_{k}t_k\log y_k}$$

- **미니배치(mini-batch)**는 훈련 데이터로부터 일부만 골라 학습하는 것을 말한다. 미니배치를 사용하는 이유는 모든 데이터를 대상으로 손실 함수의 합을 구하려면 시간이 걸리기 때문이다.
- 손실 함수를 지표로 하는 이유는 매개변수가 조금만 바뀌어도 정확도에 비해서 연속적으로 변하기 때문이다.
- **수치 미분**은 **근사치**로 계산하는 방법이다.
    - 근사치를 구하기 위해 $h$는 매우 작은 값을 사용하지만 반올림 오차 문제를 일으키기 때문에 $h = 10^{-4}$ 정도의 값을 사용한다.
    - 기울기를 구할 때 $x$에서의 기울기를 구해야 하지만 실제로는 $f(x+h)$와 $f(x)$ 사이의 기울기를 구하는 것이 된다. 따라서 이 오차를 줄이기 위해 $f(x+h)$와 $f(x-h)$일 때의 함수 f의 차분을 계산하는 방법을 사용하기도 한다. 이를 **중심 차분** 또는 **중앙 차분**이라고 한다.
- **기울기(gradient)**는 모든 변수의 편미분을 벡터로 정리한 것이다.
- 벡터장에서 기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향이다.
- **경사법(gradient method)**는 기울어진 방향으로 일정 거리만큼 이동하여 기울기를 구하는 것을 반복하여 함수의 값을 점차 줄이는 방법이다. 경사법은 기계학습을 최적화하는 데 흔히 쓰는 방법이다.

$$x_0=x_0-\eta\displaystyle\frac {\partial f}{\partial x_0} \\
x_1=x_1-\eta\displaystyle\frac {\partial f}{\partial x_1}$$

- 기호 $**\eta$(eta, 에타)**는 갱신하는 양을 나타낸다. 이를 신경망 학습에서는 **학습률(learning rate)**라고 한다.
- 학습률과 같은 매개변수를 **하이퍼파라미터(Hyper Parameter, 초매개변수)**라고 하며, 이 매개변수는 사람이 직접 설정해주어야 한다.
- 신경망에서의 기울기는 가중치 매개변수에 대한 손실 함수의 기울기를 이야기 하며, 가중치가 $W$, 손실 함수가 $L$인 신경망일 때 경사는 $\frac {\partial L} {\partial W}$로 나타낼 수 있다.
- 학습 알고리즘 구현하기
    - 전제 : 신경망에는 적응 가능한 자궁치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라고 한다.
    - 1단계 - 미니배치

        훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 목표이다.

    - 2단계 - 기울기 산출

        미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기의 손실 함수 값을 가장 작게 하는 방향을 제시한다.

    - 3단계 - 매개변수 갱신

        가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.

    - 4단계 - 반복

        1~3단계를 반복한다.

- 데이터를 미니배치로 무작위로 선정하기 때문에 **확률적 경사 하강법(Stochastic Gradient Descent, SGD)**라고 부른다.
- 에폭(epoch)은 하나의 단위이다. 1에폭은 학습에서 훈련 데이터를 모두 소진했을 때의 횟수에 해당된다.